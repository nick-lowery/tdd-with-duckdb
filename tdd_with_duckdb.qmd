---
title: "tdd-with-duckdb"
author: "Nick Lowery"
format: html
---

# the use case

This post is a very specific solution to a very specific problem, and is very much a breadcrumb for future me.  However, I imagine that there are at least a few folks out there that are in a similar situation with similar frustrations, so I'll put this out into the ether in the unlikely event someone finds it helpful.

I spend a lot of time building data pipelines, by which I mean I need to take some remote parquet files, apply various transformation and business logic to it, and write the results back to where it can be read by someone else.  My specific problem is that I find developing SQL tedious,  tuning Spark frustrating, and am enamored with the ergonomics and power of [R's package development workflows](https://r-pkgs.org/whole-game.html).

![Such parameters. Much config. Wow.](images/dataops.jpg){fig-alt="the butterfly meme, where a figure labeled "me" is looking at a butterfly labeled "randomly permuting spark configs until it works", and asking the question "is this data ops?"}

So, what I want is a way to leverage package development workflows to build data pipelines.  Great!  Instead of SQL, I can write transformation functions with \{dplyr\}, I can create fixtures and write tests and documentation for those functions, and use all the great functionality provided by \{devtools\} and friends and the RStudio IDE.  We're off and running.  

But when I go to check the performance of my functions against production data, I get hit with a bunch of errors.  The \{lubridate\} function I've used everywhere isn't implemented.  Function arguments are in a different order.  Variables aren't the right type for the function I'm calling.  And I realize that my test fixtures which I've been developing against are local R dataframes and my functions behave quite differently when applied to dataframes vs. remote tibbles due to SQL translation.

It's the classic problem of my dev environment not being close enough to my production environment, in this case specifically as it applies to test fixtures for data pipelines (hey, I warned you it was a specific problem).  So what I'm going to do here is demonstrate such a workflow, using the excellent \{duckdb\} package as my data processing engine.

## Why DuckDB?



# the workflow

## setup

### environment
- pak / renv
- duckdb
- tidyverse

## tests

### expectations

#### input data

#### output data

### fixtures

porque no dos?

#### synthetic data

#### production samples

## functions

### iterating

### bonus: documentation

# summary
